{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNGwNhZFYDhE"
   },
   "source": [
    "# Serverless Example\n",
    "## MXNet-Gluon [CLASSIFICATION]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqWvcF2p5XWQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "XEOXLqSyT2Ap",
    "outputId": "79347c1e-f966-4cdd-b6ea-f037afebacc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
      "Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.14.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2019.6.16)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "# Install some dependencies\n",
    "!pip install mxnet\n",
    "\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the ConTeXt (CPU or GPU)\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSK1CO-O_IdH"
   },
   "source": [
    "### Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ja3-ZEDT7kFL"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1N71slD65Eu4"
   },
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tn50X6xw5IxJ"
   },
   "source": [
    "### DataLoaders definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "r0dWCplEjt-8",
    "outputId": "fcca9d03-ac58-48e8-d33a-91cbbd27ead7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples    :   120\n",
      "Validation examples  :    30\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "print('Training examples    : {:5d}'.format(len(X_train)))\n",
    "print('Validation examples  : {:5d}'.format(len(X_valid)))\n",
    "\n",
    "train_ds = mx.gluon.data.DataLoader(mx.gluon.data.dataset.ArrayDataset(mx.nd.array(X_train), \n",
    "                                                                       mx.nd.array(y_train)),\n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    shuffle=True)\n",
    "\n",
    "valid_ds = mx.gluon.data.DataLoader(mx.gluon.data.dataset.ArrayDataset(mx.nd.array(X_valid), \n",
    "                                                                       mx.nd.array(y_valid)),\n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mHw7MnCo4tf"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddyvSCrbYRa9"
   },
   "outputs": [],
   "source": [
    "class Net(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.fc1 = gluon.nn.Dense(5)\n",
    "            self.fc2 = gluon.nn.Dense(4)\n",
    "            self.fc3 = gluon.nn.Dense(3)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        # F is a function space that depends on the type of x\n",
    "        \n",
    "        # Input size is inferred from the first computing\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # No need of softmax because it is in the loss function\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grFBmZ4Eh0bc"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.cast(dtype='float32')\n",
    "model.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43Rksdr84_0D"
   },
   "source": [
    "### Training\n",
    "The model will give bad performances due to the lack of normalization of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcImySOZvMUA"
   },
   "outputs": [],
   "source": [
    "def eval(data_loader, model):\n",
    "    # Cumulative loss\n",
    "    cum_loss = 0\n",
    "    num_examples = 0\n",
    "    \n",
    "    for i, (data, label) in enumerate(data_loader):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        num_examples += data.shape[0]\n",
    "        \n",
    "        y_pred = model(data)\n",
    "        loss = softmax_cross_entropy(y_pred, label)\n",
    "        cum_loss += mx.nd.sum(loss).asscalar()\n",
    "    return cum_loss/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "QGMbsHi2mQVz",
    "outputId": "0ceaab71-96eb-4929-85a2-6b3d7d2527d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 [0.05 sec] - Training Loss: 1.1146 - Validation Loss: 1.1181\n",
      "Epoch   2 [0.04 sec] - Training Loss: 1.1003 - Validation Loss: 1.1025\n",
      "Epoch   3 [0.05 sec] - Training Loss: 1.0981 - Validation Loss: 1.1023\n",
      "Epoch   4 [0.04 sec] - Training Loss: 1.0977 - Validation Loss: 1.1034\n",
      "Epoch   5 [0.04 sec] - Training Loss: 1.0973 - Validation Loss: 1.1050\n",
      "Epoch   6 [0.05 sec] - Training Loss: 1.0971 - Validation Loss: 1.1062\n",
      "Epoch   7 [0.04 sec] - Training Loss: 1.0967 - Validation Loss: 1.1077\n",
      "Epoch   8 [0.05 sec] - Training Loss: 1.0965 - Validation Loss: 1.1090\n",
      "Epoch   9 [0.04 sec] - Training Loss: 1.0963 - Validation Loss: 1.1101\n",
      "Epoch  10 [0.05 sec] - Training Loss: 1.0961 - Validation Loss: 1.1112\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "MODEL_PATH = 'model'\n",
    "\n",
    "train_loss = []    \n",
    "valid_loss = []\n",
    "\n",
    "# Time to train\n",
    "for e in range(EPOCHS):\n",
    "    tick = time.time()\n",
    "    \n",
    "    # Batch training\n",
    "    for i, (data, label) in enumerate(train_ds):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "\n",
    "        # Backpropagation\n",
    "        with autograd.record():\n",
    "            y_pred = model(data)\n",
    "            loss = softmax_cross_entropy(y_pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step(data.shape[0])\n",
    "\n",
    "    # Training metrics\n",
    "    train_loss.append(eval(train_ds, model))\n",
    "    \n",
    "    # Validation metrics\n",
    "    valid_loss.append(eval(valid_ds, model))\n",
    "    \n",
    "    print('Epoch {:3d} [{:.2f} sec] - Training Loss: {:.4f} - Validation Loss: {:.4f}'.format(e+1, time.time()-tick, train_loss[e], valid_loss[e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxWwh66w5Pea"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5KITNQPAz9uH",
    "outputId": "7b75769b-c70f-47b8-9dc2-4af708f4cc03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch : 2\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "best_epoch = valid_loss.index(min(valid_loss))\n",
    "model.export(MODEL_PATH, epoch=best_epoch)\n",
    "print(f'Best epoch : {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQoRvmU25R7R"
   },
   "source": [
    "## Testing Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XdM_vArrFMA5"
   },
   "source": [
    "### Blank Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0rRfJNeFHyx"
   },
   "outputs": [],
   "source": [
    "# Lets put us in blank paper condition\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZdD2l2XeSFf"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "vJOQxX36FHce",
    "outputId": "e83232d8-9b0b-4f73-9324-0d81664d2c6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  import sys\n",
      "predicted: ['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = logging.getLogger('iris')\n",
    "\n",
    "\n",
    "## Prediction \n",
    "def handle(event, **kwargs):\n",
    "    # If data is received as json convert to pandas\n",
    "    event = event['data'] if 'data' in event else event\n",
    "    if not isinstance(event, pd.DataFrame):\n",
    "        event = pd.DataFrame.from_dict(event, orient='columns')\n",
    "\n",
    "    # Convert to NDArray\n",
    "    data = mx.nd.array(event.values)\n",
    "    \n",
    "    # Retrieve model from disk and use it for predictions\n",
    "    model = gluon.SymbolBlock.imports(f'{MODEL_PATH}-symbol.json', ['data'], '{}-{:04d}.params'.format(MODEL_PATH, best_epoch))\n",
    "    \n",
    "    # Target format convertion\n",
    "    target_dict = {0: 'setosa', 1: 'versicolor', 2:'virginica'}\n",
    "    to_target = np.vectorize(lambda x: target_dict[x])\n",
    "    \n",
    "    return to_target(np.argmax(model(data).asnumpy(), axis=1)).tolist()\n",
    "\n",
    "## Testing and liveness check\n",
    "def test(data, **kwargs):\n",
    "    pred = handle(data)\n",
    "\n",
    "    logger.warning(f\"predicted: {pred}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "test(iris.data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mxnet_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
